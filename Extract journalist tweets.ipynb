{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlalchemy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_KEY = \"Vq4DUM39VkApxbBT6yu7Wh1BJ\"\n",
    "CONSUMER_SECRET = \"B0QISDJKan2oP9tya8erKimHuOIGIAywBQpGItVQdcjigz8zMC\"\n",
    "ACCESS_KEY = \"1092997329328562176-lJYHYuZ1W09GXzybI2ZgQV2QI2GX9k\"\n",
    "ACCESS_SECRET = \"0f8uhaUwOakDmNj5ilJdCYszxbpZ2DmDN4hIOd7GmCz4V\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql+psycopg2://postgres:stats170a@localhost/postgres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_KEY, ACCESS_SECRET)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_journalists(news_source,staff_count):\n",
    "    members = []\n",
    "    custom_id = 2000\n",
    "    for ns, list_name, party,lean in news_source:\n",
    "        i = 0\n",
    "        for page in tweepy.Cursor(api.list_members,ns,list_name).items():\n",
    "            if page.statuses_count < 200:\n",
    "                pass\n",
    "            elif i < staff_count:\n",
    "                custom_id = custom_id +1\n",
    "                members.append((custom_id,page.name,page.screen_name,ns,party, lean))\n",
    "                i = i+1\n",
    "            else:\n",
    "                break\n",
    "    return members\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(username):\n",
    "    \n",
    "    num_tweets = 200\n",
    "    \n",
    "    \n",
    "    api_tweet_list = api.user_timeline(id = username,\n",
    "                                   count = num_tweets,\n",
    "                                   tweet_mode=\"extended\",\n",
    "                                   wait_on_rate_limit=True,\n",
    "                                   wait_on_rate_limit_notify=True,)\n",
    "    \n",
    "    tweet_list = []\n",
    "    \n",
    "    for tweet in api_tweet_list:\n",
    "        tweet_list.append(tweet.full_text)\n",
    "    \n",
    "    return tweet_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_insertion(journalists):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    #stopword list\n",
    "    with open('C:/Users/Jacky Hou/Desktop/170A_HW_data/170AB PROJECT/stopwords.txt','r') as f:\n",
    "        stop_words = set([line.rstrip() for line in f])\n",
    "    stop_words = stop_words | set(stopwords.words('english'))\n",
    "    \n",
    "    for custom_id,name,twitter_handle,ns,party,lean in journalists:\n",
    "        #Insert journalist\n",
    "        value = (custom_id,twitter_handle,name,ns,party,lean)\n",
    "        engine.execute(\"INSERT INTO journalist_table VALUES (%s, %s, %s, %s, %s, %s)\", value)\n",
    "        \n",
    "        try:\n",
    "            twitter_data = get_tweets(twitter_handle)\n",
    "        except:\n",
    "            twitter_data = []\n",
    "        \n",
    "        #insert tweet\n",
    "        for tweet in twitter_data:\n",
    "            value1 = (custom_id,\n",
    "                 twitter_handle,\n",
    "                 tweet)\n",
    "            engine.execute(\"INSERT INTO journalist_tweets VALUES (%s, %s, %s)\", value1)\n",
    "        \n",
    "            #Remove stopwords\n",
    "            filtered_list = []\n",
    "            tokenlist = tokenizer.tokenize(tweet)\n",
    "            for token in tokenlist:\n",
    "                token = token.lower()\n",
    "                if token not in stop_words:\n",
    "                    filtered_list.append(token)\n",
    "        \n",
    "        #Insert bigram in table\n",
    "            for bigram in list(nltk.bigrams(filtered_list)):\n",
    "                value2 = (custom_id,\n",
    "                              twitter_handle,\n",
    "                              bigram)\n",
    "                engine.execute(\"INSERT INTO journalist_bigrams VALUES (%s, %s, %s)\", value2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_source = [\n",
    "    ('nytimes','nyt-journalists','D', 'd_solid'),\n",
    "    ('washingtonpost','washington-post-people','D','d_lean'),\n",
    "    ('WSJ','wsj-staff', 'N','N'),\n",
    "    ('WashTimes','twtopinion', 'R','r_lean'),\n",
    "    ('FoxNewsOpinion','fox-forum-contributors', 'R','r_solid')\n",
    "]\n",
    "\n",
    "staff_count = 20\n",
    "\n",
    "journalists = get_journalists(news_source,staff_count)\n",
    "\n",
    "tweet_insertion(journalists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
